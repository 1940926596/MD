## Layer Normalization

###  code

## Batch Normalization

[神经网络中的常用算法-BN算法-CSDN博客](https://blog.csdn.net/kupepoem/article/details/127351959)

**引言**

传统的神经网络，只是在将样本x输入到输入层之前对x进行标准化处理，以降低样本间的差异性。BN是在此基础上，不仅仅只对输入层的输入数据x进行标准化，还对每个隐藏层的输入进行标准化。

我们在图像预处理过程中通常会对图像进行标准化处理，也就是image normalization，使得每张输入图片的数据分布能够统均值为u，方差为h的分布。这样能够加速网络的收敛。但是当一张图片输入到神经网络经过卷积计算之后，这个分布就不会满足刚才经过image normalization操作之后的分布了，可能适应了新的数据分布规律，这个时候将数据接入激活函数中，很可能一些新的数据会落入激活函数的饱和区，导致神经网络训练的梯度消失，如下图所示当feature map的数据为10的时候，就会落入饱和区，影响网络的训练效果。这个时候我们引入Batch Normalization的目的就是使我们卷积以后的feature map满足均值为0，方差为1的分布规律。在接入激活函数就不会发生这样的情况。

![img](../Image/91761a6bb9b02bbec87e93d65d34d201.png)

**Convariate shift**
        Convariate shift是BN论文作者提出来的概念，指的是具有不同分布的输入值对深度网络学习的影响。当神经网络的输入值的分布不同时，我们可以理解为输入的特征    **值的scale**    差异较大，与权重进行矩阵相乘后，会产生一些偏离较大的差异值；而深度学习网络需要通过训练不断更新完善，那么差异值产生的些许变化都会深深影响后层，偏离越大表现越为明显；因此，对于反向传播来说，这些现象都会导致梯度发散，从而需要更多的训练步骤来抵消scale不同带来的影响，也就是说，这种分布不一致将减缓训练速度。

        而BN的作用就是将这些输入值进行标准化，降低scale的差异至同一个范围内。这样做的好处在于一方面提高梯度的收敛程度，加快模型的训练速度；另一方面使得每一层可以尽量面对同一特征分布的输入值，减少了变化带来的不确定性，也降低了对后层网络的影响，各层网络变得相对独立，缓解了训练中的梯度消失问题。
**减少内部协变量的转移**

从字面意思看来Batch Normalization（简称BN）就是对每一批数据进行归一化，确实如此，对于训练中某一个batch的数据{x1,x2,...,xn}，注意这个数据是可以输入也可以是网络中间的某一层输出。在BN出现之前，我们的归一化操作一般都在数据输入层，对输入的数据进行求均值以及求方差做归一化，但是BN的出现打破了这一个规定，我们可以在网络中任意一层进行归一化处理，因为我们现在所用的优化方法大多都是min-batch SGD，所以我们的归一化操作就成为Batch Normalization。

我们知道网络一旦train起来，那么参数就要发生更新，除了输入层的数据外(因为输入层数据，我们已经人为的为每个样本归一化)，后面网络每一层的输入数据分布是一直在发生变化的，因为在训练的时候，前面层训练参数的更新将导致后面层输入数据分布的变化。以网络第二层为例：网络的第二层输入，是由第一层的参数和input计算得到的，而第一层的参数在整个训练过程中一直在变化，因此必然会引起后面每一层输入数据分布的改变。我们把网络中间层在训练过程中，数据分布的改变称之为：“Internal Covariate Shift”。BN的提出，就是要解决在训练过程中，中间层数据分布发生改变的情况。

### code



## Droupout（暂退法，丢弃法）

**隐藏全连接层的输出上**

`torch.dropout` 是 PyTorch 中用于实现**dropout**操作的函数，常用于神经网络训练过程中的正则化，以减少过拟合的风险。dropout会随机“丢弃”一部分神经元的输出，将其置为零。具体来说，**在训练期间**，每个神经元输出值都会以设定的概率被置为 0。这个操作能够帮助模型更好地学习多样化的特征表示，提高模型的泛化能力。

**语法**

```python
torch.dropout(input, p, training)
```

- `input`: 输入张量，表示神经网络层的输出。
- `p`: dropout的概率，取值范围在 [0, 1) 之间。`p` 表示置零的概率，例如 `p=0.5` 表示有 50% 的输出将被随机置零。
- `training`: 布尔值，表示当前是否处于训练模式。只有当 `training=True` 时，dropout 才会生效；在推理（测试）模式下，应设置为 `False`，避免对输出进行dropout。

**返回值**

返回一个新的张量，表示经过 dropout 操作后的结果。对于被保留下来的部分输出，会自动进行放缩（放大 $\frac{1}{1-p}$ 倍），以保证输出的期望值与未进行 dropout 时一致。

**示例**

```python
import torch

# 创建一个示例张量
x = torch.tensor([[1.0, 2.0, 3.0],
                  [4.0, 5.0, 6.0]])

# 应用 dropout，设定置零概率为 0.5，并开启训练模式
output = torch.dropout(x, p=0.5, training=True)
print(output)
```

在以上示例中，每个元素有 50% 的概率被置为 0，而其余未被置零的元素会放大 $\frac{1}{1 - 0.5} = 2$ 倍。示例输出可能为：

```
tensor([[2.0, 0.0, 6.0],
        [8.0, 0.0, 12.0]])
```

> **注意**：输出是随机的，每次运行可能会有不同的零值位置。

**注意事项**

1. **训练和推理模式**：在训练模式（`training=True`）下使用 dropout 是有效的，而在推理模式（`training=False`）下 dropout 不会影响输出，以确保在测试时模型行为的一致性。
2. **放缩**：dropout 会对未被置零的输出进行放缩，保持**期望输出**的一致性。这意味着输出张量的均值不会因为 dropout 而改变。

在 Python 中，`*` 运算符在不同上下文中有不同的作用。在 `shape` 这样的元组前面加上 `*`，可以用于**参数解包**，将元组或列表的元素作为单独的参数传递给函数或用于其他表达式中。

## * 在形状元组前的作用

假设我们有一个 `shape` 元组，表示张量的形状，例如 `(2, 3, 4)`：

```python
shape = (2, 3, 4)
```

1. **函数参数解包**：

   如果我们想要将 `shape` 中的每个元素分别传递给 `torch.randn` 函数以生成一个随机张量，可以这样写：

   ```python
   import torch
   tensor = torch.randn(*shape)
   print(tensor.shape)  # 输出: torch.Size([2, 3, 4])
   ```

   这里，`*shape` 会将 `(2, 3, 4)` 解包成三个独立的参数 `2, 3, 4`，相当于 `torch.randn(2, 3, 4)`。

2. **列表或元组构建**：

   `*` 也可以用于构建新的列表或元组，例如：

   ```python
   new_shape = (1, *shape, 1)
   print(new_shape)  # 输出: (1, 2, 3, 4, 1)
   ```

   这里，`*shape` 被解包，放入到新的元组中，使得 `new_shape` 的值为 `(1, 2, 3, 4, 1)`。

**总结**

- `*shape` 会将 `shape` 中的**每个元素解包为独立的值**。

- 这种操作广泛用于函数调用中，能够简化代码并增加灵活性。

---

这个警告信息的意思是：当你从一个已经是 `tensor` 的对象构造一个新的 `tensor` 时，使用 `torch.tensor()` 可能会导致一些意料之外的行为，特别是在涉及到梯度计算时。因此，PyTorch 推荐使用 `tensor.clone().detach()` 或者 `tensor.clone().detach().requires_grad_(True)` 来进行复制操作，以确保代码的稳定性和预期行为。

## tensor clone

**问题来源**

假设你有一个 `tensor` 对象，比如 `tensor` 是一个已经存在的 `Tensor`：

```python
import torch

tensor = torch.randn(2, 2, requires_grad=True)
```

如果你执行：

```python
new_tensor = torch.tensor(tensor)
```

这行代码会创建一个新的 `tensor`，但它不会保留原始 `tensor` 的梯度信息。具体来说：
- `torch.tensor(tensor)` 会创建一个新的 `tensor`，但是这个新的 `tensor` 不会保持与原始 `tensor` 之间的梯度连接关系。**（无视图关系）**
- 如果原始 `tensor` 是一个带有 `requires_grad=True` 的 `tensor`，而你用 `torch.tensor()` 创建的新 `tensor` 则会丢失这个梯度计算的连接。

**推荐做法**

PyTorch 推荐使用以下方法来复制 `tensor`，确保新的 `tensor` 保留正确的属性：

1. **`tensor.clone().detach()`**：这会创建一个新的 `tensor`，并且断开它与原始 `tensor` 的计算图连接。适用于你不想计算梯度时。
   ```python
   new_tensor = tensor.clone().detach()
   ```

2. **`tensor.clone().detach().requires_grad_(True)`**：如果你需要创建一个新的 `tensor`，并且希望它继续跟踪梯度计算，可以使用这个方法。
   ```python
   new_tensor = tensor.clone().detach().requires_grad_(True)
   ```

**为什么需要这样做？**

1. **避免梯度问题**：当你直接使用 `torch.tensor(tensor)` 时，新的 `tensor` 会丢失与原始 `tensor` 的梯度计算关联。而使用 `.clone().detach()` 确保了新的 `tensor` 不会无意中连接到计算图中，避免潜在的梯度计算错误。

2. **明确的内存管理**：`clone()` 方法显式地创建了一个新的副本，因此内存和数据不会被直接共享，这对于避免不必要的内存共享或副作用非常重要。

**示例代码**

```python
import torch

# 创建一个有梯度的张量
tensor = torch.randn(2, 2, requires_grad=True)

# 错误方式，会触发警告
new_tensor = torch.tensor(tensor)  # 警告：直接通过 torch.tensor() 创建新张量

# 推荐方式，避免梯度丢失
new_tensor_correct = tensor.clone().detach()

# 如果需要新 tensor 仍然能够计算梯度
new_tensor_with_grad = tensor.clone().detach().requires_grad_(True)
```

总结：通过 `clone().detach()` 或 `clone().detach().requires_grad_(True)` 来复制 `tensor`，可以避免不必要的计算图问题，并确保代码的稳定性。