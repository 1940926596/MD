#  一元线性回归模型

## 设定参数

```python
model=Model()

#总迭代次数
epochs=1000

#学习率
learning_rate=0.01

print(model.parameters())

#优化器
optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate)

#损失函数
criterion=nn.MSELoss() 
```

1. `epochs=1000`：
   - 这行代码定义了训练过程中的总迭代次数，即整个训练集将被重复使用1000次来更新模型的参数。一个“epoch”指的是将所有的训练数据完整地用一遍。

2. `learning_rate=0.01`：
   - 学习率是一个超参数，它控制着在优化过程中参数更新的步长大小。较小的学习率可能使训练过程更稳定，但训练速度可能会较慢；较大的学习率可能加快训练速度，但也可能导致结果不稳定或甚至发散。

3. `print(model.parameters())`：
   - 这行代码输出模型的参数。在PyTorch中，`model.parameters()`是一个生成器，它返回模型中所有可训练的参数（比如权重和偏差）。这通常用于调试目的，以查看参数的初始状态或确认参数的结构。

4. `optimizer=torch.optim.SGD(model.parameters(), lr=learning_rate)`：
   - 这里定义了一个优化器，用于更新模型的权重。`torch.optim.SGD`是随机梯度下降优化器的实现，其中`model.parameters()`提供了需要优化的参数，`lr=learning_rate`设置了学习率。随机梯度下降是一种常见的优化算法，可以有效地在数据集上训练神经网络。

5. `criterion=nn.MSELoss()`：
   - 这行代码定义了一个损失函数，这里使用的是均方误差损失（Mean Squared Error Loss），它是一种常用于回归任务的损失函数。损失函数用于计算模型的预测值和真实值之间的误差，优化器会利用这个误差来更新模型参数，目的是减少模型的总损失。

综上所述，这段代码设定了神经网络训练的基本参数，包括训练迭代次数、学习率、模型参数的查看、优化方法和损失计算方式。这些都是训练神经网络时必须进行设置的基础部分。

## 学习步骤

1. **前向传播**：首先，数据通过模型的多个层传播，每一层都会进行其相应的计算，如线性变换、激活函数等。最终，这个传播过程输出一个预测结果。
2. **计算损失**：预测结果与真实数据之间的差异通过损失函数计算出来，得到一个标量损失值（scalar loss）。这个损失值表示当前模型预测的“不准确度”。
3. **反向传播**（由 `loss.backward()` 触发）：
   - 当调用 `loss.backward()` 时，PyTorch 会回溯整个网络，自动计算相对于网络中每个可训练参数的损失梯度。这一计算是通过链式法则实现的，是微积分中的一种基本技巧。
   - 在这个过程中，每个参数（通常是权重和偏置）的梯度都会被存储在参数的 `.grad` 属性中。
4. **参数更新**：==一旦梯度被计算并存储==，优化器（比如SGD, Adam等）可以使用这些梯度来更新模型的参数，步骤通常是： `optimizer.step()`。这个更新是为了减少损失值，即 使模型的预测更加接近真实的目标数据。

```python
for epoch in range(epochs):
    
    epoch+=1
    inputs=torch.from_numpy(x_train)
    labels=torch.from_numpy(y_train)
    #转换成tensor

    #将每一次的迭代的梯度都要清零
    optimizer.zero_grad()

    #前向传播
    outputs=model(inputs)

    #计算损失函数
    loss=criterion(outputs,labels)

    #在PyTorch中，loss.backward() 是一个非常重要的操作，用于执行自动微分，即计算当前损失（loss）函数对网络参数的梯度。这个梯度在随后用于通过优化器更新模型的参数，从而使模型更好地拟合或预测数据。
    #反向传播
    loss.backward()

    #更新参数
    optimizer.step()

    if epoch % 50 ==0:
        print('epoch {},loss {}'.format(epoch,loss.item()))
```

- **Epoch（时期）：**

  当一个完整的数据集通过了神经网络一次并且返回了一次，这个过程称为一次epoch。（也就是说，所有训练样本在神经网络中都 进行了一次正向传播 和一次反向传播 ）

  再通俗一点，一个Epoch就是将所有训练样本训练一次的过程。

  然而，当一个Epoch的样本（也就是所有的训练样本）数量可能太过庞大（对于计算机而言），就需要把它分成多个小块，也就是就是分成多个Batch 来进行训练。

- **Batch（批 / 一批样本）：**

​	将整个训练样本分成若干个Batch。

- **Batch_Size（批大小）：**

​	每批样本的大小。

- **Iteration（一次迭代）：**

​	训练一个Batch就是一次Iteration
