## 读取表格数据

```python
import pandas as pd
import numpy as np

df = pd.read_excel('data.xls')

print(df.head()) # 预览数据

column1_data = df.iloc[:, 0]
print(column1_data)

print(df.shape)
# 输出df的横纵属性  结果：(4, 3)

# 定义原始数组
arr = np.array([[1, 1, 1], [1, 0, 1], [0, 1, 0], [0, 0, 0]])

# 获取第一、二列
new_arr = arr[:, 0:2]

# 打印第一、二列
print(new_arr)

# 获取最后一列
last_column = arr[:, 2:3]

# 打印最后一列
print(last_column)

```

`arr[:, 2:3]` 和 `arr[:, 2]` 都用于获取数组 `arr` 的第 3 列，但它们在形状上有所不同：

1. **arr[:, 2:3]**:
   
   - 获取的是一个二维数组。
   - `:` 表示选择所有行，`2:3` 表示从第 3 列开始，直到（但不包括）第 4 列。由于只有第 3 列满足这个条件，结果是一个二维数组，形状为 `(n, 1)`，其中 `n` 是行数。
   
   ```python
   array([[1],
          [1],
          [0],
          [0]])
   ```
   形状为 `(4, 1)`。
   
2. **arr[:, 2]**:
   
   - 获取的是一个一维数组。
   - `:` 表示选择所有行，`2` 表示选择第 3 列（索引从 0 开始）。结果是一个一维数组，形状为 `(n,)`。
   
   ```python
   array([1, 1, 0, 0])
   ```
   形状为 `(4,)`。

**总结**: `arr[:, 2:3]` 保留了二维数组的性质，而 `arr[:, 2]` 则会将其简化为一维数组。

## 定义神经网络

```python
import torch
import torch.nn as nn
import pandas as pd

# 定义神经网络
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(2, 5)  # 输入层到隐藏层
        self.fc2 = nn.Linear(5, 1)  # 隐藏层到输出层

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# 定义训练数据
x_train = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)
y_train = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)

# 实例化神经网络
net = Net()

# 定义损失函数和优化器
criterion = nn.MSELoss()

optimizer = torch.optim.SGD(net.parameters(), lr=0.1)

# 训练网络
for epoch in range(1000):
    optimizer.zero_grad()  # 梯度清零
    y_pred = net(x_train)  # 前向传播
    loss = criterion(y_pred, y_train)  # 计算损失
    loss.backward()  # 反向传播
    optimizer.step()  # 更新参数

# 测试网络
x_test = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)
y_pred = net(x_test)
print(y_pred)
```
