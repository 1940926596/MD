# 强化学习

## Chapter1 基本概念

### 状态（状态空间State）

![image-20250508140127763](../../Image/image-20250508140127763.png)

### 动作（Action）

![image-20250508140250188](../../Image/image-20250508140250188.png)

### 状态转换（State Transition）

![image-20250508141612224](../../Image/image-20250508141612224.png)

**以上表格表示状态转移是确定的例子**

**状态转换随机化**

![image-20250508145738608](C:/Users/19409/AppData/Roaming/Typora/typora-user-images/image-20250508145738608.png)

### 策略（Policy） 

![image-20250508145954241](C:/Users/19409/Desktop/MD/Image/image-20250508145954241.png)

**每一个状态都会有策略**

![image-20250508150205666](C:/Users/19409/Desktop/MD/Image/image-20250508150205666.png)

![image-20250508150536736](C:/Users/19409/Desktop/MD/Image/image-20250508150536736.png)

### 奖励（Reward）

![image-20250508151523692](C:/Users/19409/Desktop/MD/Image/image-20250508151523692.png)

![image-20250508151814814](C:/Users/19409/Desktop/MD/Image/image-20250508151814814.png)

![image-20250508152304636](C:/Users/19409/Desktop/MD/Image/image-20250508152304636.png)

### Trajectory（轨迹）和Return 

![image-20250508152613345](C:/Users/19409/Desktop/MD/Image/image-20250508152613345.png)

![image-20250508152550314](C:/Users/19409/Desktop/MD/Image/image-20250508152550314.png)

**discounted return** 

![image-20250508153540117](C:/Users/19409/Desktop/MD/Image/image-20250508153540117.png)

![image-20250508153518437](C:/Users/19409/Desktop/MD/Image/image-20250508153518437.png)

### Episode（finite trajectory）

![image-20250508160601824](C:/Users/19409/Desktop/MD/Image/image-20250508160601824.png)

![image-20250508161248175](C:/Users/19409/Desktop/MD/Image/image-20250508161248175.png)

### MDP

![image-20250508161946511](C:/Users/19409/Desktop/MD/Image/image-20250508161946511.png)

![image-20250508162049592](C:/Users/19409/Desktop/MD/Image/image-20250508162049592.png)

**马尔科夫过程和马尔科夫决策过程**

>![image-20250508220044294](C:/Users/19409/Desktop/MD/Image/image-20250508220044294.png)
>
>![image-20250508220050211](C:/Users/19409/Desktop/MD/Image/image-20250508220050211.png)



## Chapter2 贝尔曼公式

**为什么return重要**

![image-20250508170216763](C:/Users/19409/Desktop/MD/Image/image-20250508170216763.png)

![image-20250508170721788](C:/Users/19409/Desktop/MD/Image/image-20250508170721788.png)

### bellman公式

**计算return**

![image-20250508173109705](C:/Users/19409/Desktop/MD/Image/image-20250508173109705.png)

![image-20250508173245145](C:/Users/19409/Desktop/MD/Image/image-20250508173245145.png)

### State Value

### Bellman Equation