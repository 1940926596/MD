## 非生成模型

非生成模型主要关心数据的**表示或聚类**，无需建模$p(x)$ 或$p(x,z)$。其目标是直接挖掘数据结构或寻找数据的低维表示。

- $x$ 表示观测数据。

- $z$ 表示潜在变量（如果有的话）。

### **特点**

1. **建模重点不同**：
   - 关注数据的几何结构或统计关系，而非完整的生成机制。
   - 不建模联合分布，通常优化直接与数据相关的目标函数。
2. **无需生成样本**：
   - 非生成模型不需要生成新样本，而是专注于描述或分析已有数据。
3. **效率较高**：
   - 不需要复杂的积分或采样过程，模型的计算通常较生成模型简单。

### **典型模型**

- 聚类模型：
  - $K$-均值（$K$-means）：直接基于欧几里得距离将数据分为 KK个簇。
  - 层次聚类：构建数据的分层聚类树。
- 降维模型：
  - 主成分分析（$PCA$）：通过线性变换找到数据的低维表示。
  - 核$PCA$：利用核方法处理非线性关系。

## 编码器

### 自编码器

![image-20240928182148797](../../Image/image-20240928182148797.png)

**两层网络结构的自编码器**

![image-20240928182413531](C:/Users/19409/AppData/Roaming/Typora/typora-user-images/image-20240928182413531.png)

**稀疏编码**



**稀疏自编码器**



![image-20240928182457469](../../Image/image-20240928182457469.png)

**降噪自编码器Denoising autoencoder**

![image-20240928183008994](../../Image/image-20240928183008994.png)

## 密度估计-深度生成网络

生成模型通过建模数据的联合分布 $p(x,z)$ 或 $p(x)$，其中：$x $表示观测数据。$z$ 表示潜在变量（如果有的话）。其目标是学习到数据的分布结构，从而能够生成新样本。**密度估计**：拟合输入数据的边缘分布 $P(x)$。

深度生成网络是一类机器学习模型，旨在生成与训练数据相似的数据。这些网络在图像生成、文本生成、音乐生成等领域有广泛应用。常见的深度生成网络包括变分自动编码器（VAE）、生成对抗网络（GAN）以及结合两者优点的VAE-GAN。

### 变分自编码器(VAE)

变分自编码器（Variational Autoencoder, VAE）是一种生成模型，其目标是学习数据的潜在分布 $ p(x) $，并可以从中生成新的样本。为了实现这一点，我们需要最大化数据的对数边际似然 $ \log p(x) $。

#### 1. 问题背景

变分自编码器（Variational Autoencoder, VAE）是一种生成模型，其目标是学习数据的潜在分布 $ p(x) $，并可以从中生成新的样本。为了实现这一点，我们需要最大化数据的对数边际似然 $ \log p(x) $。

#### 2. 数据生成过程和模型假设

VAE 的生成过程可以描述为以下两步：

1. 从先验分布 $ p(z) $ 中采样潜在变量 $ z $；
2. 根据条件概率 $ p(x|z) $ 从潜在变量生成观测数据 $ x $。

因此，数据的边际分布为：
$
p(x) = \int p(x|z)p(z) \, dz
$

#### 3. 变分下界

直接计算 $ \log p(x) $ 通常不可行，因为积分 $ \int p(x|z)p(z) \, dz $ 无法解析。为此，VAE 引入了一个近似后验分布 $ q(z|x) $，并使用变分推断优化。

对数边际似然可以分解为两部分：
$
\log p(x) = \mathbb{E}_{q(z|x)} \left[ \log \frac{p(x, z)}{q(z|x)} \right] + D_{\mathrm{KL}}(q(z|x) \| p(z|x))
$

其中：

- 第一项 $ \mathbb{E}_{q(z|x)} \left[ \log \frac{p(x, z)}{q(z|x)} \right] $ 是变分下界（ELBO, Evidence Lower BOund）；
- 第二项 $ D_{\mathrm{KL}}(q(z|x) \| p(z|x)) $ 是 KL 散度，表示近似后验与真实后验的差异。

由于 KL 散度是非负的，可以得到以下不等式：
$
\log p(x) \geq \mathbb{E}_{q(z|x)} \left[ \log \frac{p(x, z)}{q(z|x)} \right]
$

为了优化 $ \log p(x) $，我们最大化 ELBO：
$
\mathrm{ELBO} = \mathbb{E}_{q(z|x)} \left[ \log p(x|z) \right] - D_{\mathrm{KL}}(q(z|x) \| p(z))
$

#### 4. 损失函数

从 ELBO 的定义可以推导出 VAE 的损失函数：
$
\mathcal{L}_{\text{VAE}} = - \mathbb{E}_{q(z|x)} [\log p(x|z)] + D_{\mathrm{KL}}(q(z|x) \| p(z))
$

- 第一项 $ - \mathbb{E}_{q(z|x)} [\log p(x|z)] $：重构误差，衡量生成数据 $ p(x|z) $ 与真实数据 $ x $ 的相似性；
- 第二项 $ D_{\mathrm{KL}}(q(z|x) \| p(z)) $：正则化项，约束 $ q(z|x) $ 接近先验分布 $ p(z) $。

#### 5. 具体步骤

##### 5.1. 编码器（Encoder）

编码器近似后验分布 $ q(z|x) $，通常假设为高斯分布 $ \mathcal{N}(z; \mu_\phi(x), \sigma^2_\phi(x)) $，均值和方差由神经网络参数化：
$
q(z|x) = \mathcal{N}(z; \mu_\phi(x), \sigma^2_\phi(x))
$

##### 5.2. 重参数化技巧

为了使采样 $ z \sim q(z|x) $ 可微，引入重参数化技巧，将 $ z $ 表示为：
$
z = \mu_\phi(x) + \sigma_\phi(x) \odot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)
$

这样，通过对 $ \epsilon $ 采样，网络的梯度可以通过 $ \mu_\phi(x) $ 和 $ \sigma_\phi(x) $ 传播。

##### 5.3. 解码器（Decoder）

解码器通过条件概率分布 $ p(x|z) $ 生成数据 $ x $，其分布参数同样由神经网络输出。

---

#### 6. 总结

VAE 通过最大化以下变分下界（ELBO）来学习数据分布：
$
\mathrm{ELBO} = \mathbb{E}_{q(z|x)} \left[ \log p(x|z) \right] - D_{\mathrm{KL}}(q(z|x) \| p(z))
$

损失函数中的两部分分别对应数据重构误差和正则化约束。其关键创新包括：

1. 使用重参数化技巧实现可微采样；
2. 引入变分推断框架优化复杂的生成模型。

通过学习潜在空间的分布，VAE 不仅可以生成高质量的样本，还能为数据提供有意义的潜在表示。

### 生成对抗网络(GAN)

### 玻尔兹曼机(BM)

### 受限玻尔兹曼机(RBM)

### 深度信念网络(DBN)

**深度置信网络组成元件是受限玻尔兹曼机。**