# 强化学习

## 马尔科夫

### 1. 马尔科夫过程 (Markov Process)

- **定义**: 一种随机过程，其未来状态只依赖于当前状态，而与过去的状态无关（无后效性）。
$$
P(X_{t+1} | X_t, X_{t-1}, \dots, X_0) = P(X_{t+1} | X_t)
$$

强化学习中的应用：
- 描述环境状态的动态转移，最简单的形式不涉及动作。
- 一个马尔可夫过程由 $ \langle S, P \rangle $ 定义，其中：
  - $ S $: 状态空间
  - $ P $: 状态转移概率



### 2. 马尔科夫链 (Markov Chain)
- **转移概率矩阵**:

$$
P = 
\begin{bmatrix}
P_{11} & P_{12} & \dots & P_{1n} \\
P_{21} & P_{22} & \dots & P_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
P_{n1} & P_{n2} & \dots & P_{nn}
\end{bmatrix}
$$

其中 $ P_{ij} = P(X_{t+1}=j | X_t=i) $。

>马尔科夫链是一种 **有向图**。其特点如下：  
>
>1. **有向图的定义**
>   - 马尔科夫链中，节点表示系统的状态。
>   - 边表示状态之间的转移，并且这些转移是有方向性的。例如，从状态 $i$ 到状态 $j$ 的转移可能存在，而从 $j$ 到 $i$ 不一定存在。
>   - 每条边上通常标注一个概率值，称为转移概率。
>
>2. **为何是有向图**
>   - 转移概率 $P(i \to j)$ 是从一个状态 $i$ 到另一个状态 $j$ 的条件概率，这个过程有方向性，不能简单地认为 $P(i \to j) = P(j \to i)$。
>   - 这种方向性使得马尔科夫链的状态转移模型天然构成一个有向图。
>
>3. **补充：马尔科夫链的矩阵表示**
>   - 马尔科夫链常用转移概率矩阵（Transition Probability Matrix）来表示，其中每个元素 $P_{ij}$ 表示从状态 $i$ 到状态 $j$ 的转移概率。
>   - 由于矩阵并不一定对称，进一步说明了马尔科夫链是有向的。
>
>4. **其他相关概念**
>   - 如果将马尔科夫链看作无向图，则会丢失转移概率的方向性，无法准确建模状态之间的动态关系。
>   - **无向图** 通常用于建模对称关系，例如马尔科夫随机场（Markov Random Field，MRF），它是一种无向图模型，与马尔科夫链不同。
>
>总结：马尔科夫链是一个 **有向图模型**，用于表示具有方向性和转移概率的状态转移过程。



### 3. 隐马尔科夫模型 (Hidden Markov Model, HMM)
- **组成部分**:
  - 初始状态概率:
  
  $$
  \pi_i = P(X_0 = i)
  $$

  - 状态转移概率:

  $$
  A_{ij} = P(X_{t+1}=j | X_t=i)
  $$

  - 观测概率:

  $$
  B_{ij} = P(O_t = o_j | X_t = i)
  $$

- **目标函数**:
  评估观测序列的概率：

  $$
  P(O | \lambda) = \sum_{X} P(O, X | \lambda)
  $$



### 4. **马尔可夫奖励过程（Markov Reward Process, MRP）**

定义：

- 在马尔可夫过程基础上，加入了奖励机制：
  $
  P(s_{t+1}, r_t | s_t) = P(s_{t+1} | s_t), r_t = R(s_t)
  $
- 描述为 $ \langle S, P, R, \gamma \rangle $，其中：
  - $ R(s) $: 状态的奖励函数
  - $ \gamma $: 折扣因子，用于计算累计奖励

强化学习中的应用：

- MRP 用于**状态值函数**的定义：
  $
  V(s) = \mathbb{E}\left[\sum_{t=0}^\infty \gamma^t r_t \mid s_0 = s\right]
  $
- 在策略评估中，计算某个固定策略的累积奖励值。



### 5. **马尔可夫决策过程（Markov Decision Process, MDP）**

- 强化学习的核心数学框架，包含了**决策**：
  $
  P(s_{t+1}, r_t | s_t, a_t) = P(s_{t+1} | s_t, a_t)
  $
  即未来状态和奖励取决于当前的状态和动作。

- 一个 MDP 由 $ \langle S, A, P, R, \gamma \rangle $ 定义，其中：
  - $ S $: 状态空间
  - $ A $: 动作空间
  - $ P(s'|s,a) $: 状态转移概率
  - $ R(s, a)$: 奖励函数
  - $ \gamma $: 折扣因子

强化学习中的应用：

- 强化学习的目标是学习一个最优策略 $ \pi^* $，使得期望累计奖励最大化：
  $
  \pi^* = \arg\max_\pi \mathbb{E}\left[\sum_{t=0}^\infty \gamma^t r_t\right]
  $

- **折扣奖励的累积形式**:

$$
G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \dots = \sum_{k=0}^\infty \gamma^k R_{t+k}
$$

- **贝尔曼方程**:

  对于状态值函数 $ V(s) $:

  $$
  V(s) = \max_a \sum_{s'} P(s' | s, a) \left[ R(s, a) + \gamma V(s') \right]
  $$



### 6. 马尔科夫随机场 (Markov Random Field, MRF)
- **联合概率分布**:

$$
P(X_1, X_2, \dots, X_n) = \frac{1}{Z} \prod_{C \in \text{cliques}} \psi_C(X_C)
$$

其中 $ \psi_C(X_C) $ 是团势函数，$ Z $ 是归一化常数。



### 7. 马尔科夫链蒙特卡罗方法 (Markov Chain Monte Carlo, MCMC)
- **详细平衡条件**:

$$
\pi(i)P_{ij} = \pi(j)P_{ji}
$$

一种基于马尔可夫链的采样方法，用于从复杂分布中生成样本。

强化学习中的应用：
- 在策略优化中，MCMC 可以用于**策略评估**或**值函数近似**。
- 深度强化学习中，策略梯度方法有时会借助 MCMC 进行经验采样。



### 8. 马尔科夫模型的重要概念

#### **无后效性 (Markov Property)**

$$
P(X_{t+1} | X_t, X_{t-1}, \dots, X_0) = P(X_{t+1} | X_t)
$$

#### **平稳分布 (Stationary Distribution)**

$$
\pi P = \pi
$$

#### **累积奖励的定义**:

$$
G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \dots
$$

#### **混合时间 (Mixing Time)**

指马尔科夫链达到平稳分布所需的时间，没有具体公式。



### 9. 隐马尔科夫模型的解码问题

- **维特比算法**:

$$
\delta_t(i) = \max_{j} \left[ \delta_{t-1}(j) A_{ji} \right] B_{i}(O_t)
$$

其中 $ \delta_t(i) $ 是最优路径的概率。
