# 线性模型

## 分类问题示例

## 线性分类模型

## 交叉熵与对数似然

### 信息论

![image-20240908210940980](../../Image/image-20240908210940980.png)

### 熵（Entropy）

![image-20240908211052395](../../Image/image-20240908211052395.png)

### 熵编码

![image-20240908211113935](../../Image/image-20240908211113935.png)

### 交叉熵

![image-20241005021001205](../../Image/image-20241005021001205.png)

### KL散度

![image-20241005021023141](../../Image/image-20241005021023141.png)

### 应用到机器学习

![image-20241005021050091](../../Image/image-20241005021050091.png)

==**下面开始具体的分类器**==

## Logistic回归

![image-20240908210813742](../../Image/image-20240908210813742.png)

## Softmax回归

> 主要学习方式，参数化的条件概率和真实的条件概率的交叉熵

![image-20241005012303474](../../Image/image-20241005012303474.png)

**softmax函数**

![image-20241005020313272](../../Image/image-20241005020313272.png)

**softmax回归**

![image-20241005020421899](../../Image/image-20241005020421899.png)

> 学习准则：交叉熵
>
> 优化方法：梯度下降

![image-20241005020906746](../../Image/image-20241005020906746.png)

## 感知器（Perceptron）

![image-20241005020834461](../../Image/image-20241005020834461.png)

## 支持向量机（SVM）

**几何距离**

![image-20241005021510666](../../Image/image-20241005021510666.png)

**支持向量机**

![image-20241005021609934](../../Image/image-20241005021609934.png)

## 朴素贝叶斯分类器

**贝叶斯分类** 是基于贝叶斯定理的一种分类方法，通过计算后验概率来做分类。

**朴素贝叶斯分类器** 是贝叶斯分类的一种简化版本，它假设特征之间条件独立，从而简化了计算。

**贝叶斯估计** 是贝叶斯推断的一种应用，侧重于对未知参数的估计，而不是分类。

![image-20250217154032281](../../Image/image-20250217154032281.png)

### 贝叶斯估计

是的，贝叶斯估计通常指的是最大后验估计（MAP, Maximum A Posteriori Estimation）。在贝叶斯框架下，估计参数时，我们结合了先验分布和观察数据得到后验分布。最大后验估计通过最大化后验分布来得到参数的估计值。

具体来说，对于一个给定的观察数据$D$，参数$\theta$的最大后验估计是通过最大化后验分布$P(\theta | D)$得到的。根据贝叶斯公式，后验分布可以表示为：

$
P(\theta | D) = \frac{P(D | \theta)P(\theta)}{P(D)}
$

其中：
- $P(D | \theta)$是似然函数，表示给定参数$\theta$时观测数据$D$的概率。
- $P(\theta)$是先验分布，表示参数$\theta$在没有数据时的分布。
- $P(D)$是数据的边际概率，通常通过优化估计的常数被忽略。

最大后验估计等价于最大化似然函数和先验分布的乘积：

$
\hat{\theta}_{MAP} = \arg \max_{\theta} P(D | \theta)P(\theta)
$

所以，贝叶斯估计就是通过这种方式找到最有可能的参数$\theta$。

如果先验分布$P(\theta)$是均匀分布或者没有先验信息，那么最大后验估计简化为最大似然估计（MLE）。

**==进一步澄清这个区别：==**

**贝叶斯分类 (Bayesian Classification)**

贝叶斯分类的目标是计算后验概率 $P(C | X)$，用来进行分类决策。这里的类别 $C$ 是我们要预测的标签，特征 $X$ 是给定的输入数据。贝叶斯分类并不涉及对模型参数（如 $\theta$）的估计，而是通过贝叶斯定理直接计算后验概率，并基于此来做分类决策。

在贝叶斯分类中，我们有：

$$
P(C | X) = \frac{P(X | C) P(C)}{P(X)}
$$

但是这个过程中，并没有涉及到对某个模型的“参数”进行估计，而是通过样本数据来计算似然 $P(X | C)$ 和先验 $P(C)$ 等。

---

**贝叶斯估计 (Bayesian Estimation)**

也叫贝叶斯推断。

贝叶斯估计涉及到通过观察数据来估计未知的模型参数 $\theta$，并且它需要使用后验分布来进行推断。在贝叶斯估计中，参数 $\theta$ 是模型的一部分，我们通过最大化后验分布来估计这些参数。

贝叶斯公式为：

$$
P(\theta | D) = \frac{P(D | \theta) P(\theta)}{P(D)}
$$

其中：

- $P(D | \theta)$ 是似然函数，表示在给定参数 $\theta$ 下观察到数据 $D$ 的概率。
- $P(\theta)$ 是参数 $\theta$ 的先验分布。
- $P(D)$ 是数据的边际似然（通常作为常数）。

贝叶斯估计的目标通常是通过最大化后验分布 $P(\theta | D)$ 或者计算后验分布的期望来估计参数 $\theta$。

---

### 总结

- **贝叶斯分类** 不涉及直接估计模型的参数 $\theta$，它通过计算后验概率来进行分类，主要关注的是类别 $C$。
- **贝叶斯估计** 则专注于通过数据来估计模型的参数 $\theta$，这种方法在统计建模中尤其常见。

所以，贝叶斯分类可以看作是没有涉及参数 $\theta$ 的分类方法，而贝叶斯估计则明确涉及到对参数 $\theta$ 的推断。


## 高斯判别分析

高斯判别分析（Gaussian Discriminant Analysis, GDA）是一种用于分类问题的统计方法，假设数据服从高斯分布（正态分布），并基于贝叶斯定理进行推理。GDA的目标是学习各个类别的概率分布，并通过这些分布来进行分类。

### 主要假设：
1. **特征条件独立性**：对于每个类别，特征被假设为高斯分布。每个类别的特征向量$x$遵循一个多元高斯分布：
   $$
   p(x | y = k) = \frac{1}{(2\pi)^{d/2} |\Sigma_k|^{1/2}} \exp\left( -\frac{1}{2} (x - \mu_k)^T \Sigma_k^{-1} (x - \mu_k) \right)
   $$
   其中，$\mu_k$是类别$k$的均值向量，$\Sigma_k$是类别$k$的协方差矩阵，$d$是特征的维度。

2. **类别的先验分布**：每个类别的先验概率$p(y=k)$是已知的或可以通过训练数据来估计。

### 分类决策：
根据贝叶斯定理，给定一个观测值$x$，我们可以计算每个类别的后验概率$p(y=k | x)$。然后，选择后验概率最大的类别作为预测类别：
$$
p(y=k | x) \propto p(x | y=k) p(y=k)
$$
最终的决策规则是：
$$
\hat{y} = \arg\max_k \, p(x | y=k) p(y=k)
$$
这相当于选择使得条件概率最大化的类别。

### 主要步骤：
1. **参数估计**：通过训练数据估计每个类别的均值向量$\mu_k$、协方差矩阵$\Sigma_k$和先验概率$p(y=k)$。
2. **分类**：通过计算每个类别的后验概率并选择最大值对应的类别进行分类。

### 优点：
- GDA对数据的分布做了明确假设（数据服从高斯分布），当数据符合这些假设时，表现较好。
- 计算上相对简单，且可以通过协方差矩阵捕捉数据之间的相关性。

### 缺点：
- 当数据不满足高斯分布假设时，GDA的性能可能会大幅下降。
- 对异常值比较敏感。
