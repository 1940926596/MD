## Normalization

**批量归一化**

- 损失出现在最后，后面的层训练较快
- 数据在最底部
  - 底部的层训练较慢
  - 底部层一变化，所有都得跟着变
  - 最后的那些层需要重新学习多次
  - 导致收敛变慢
- 我们可以在学习底部层的时候避免变化顶部层吗?

**批量归一化层**

![image-20241108132259782](..\Image\image-20241108132259782.png)

**减少协变量转移**：最初的论文提出批量归一化的目的是为了减少神经网络内部的协变量转移，即不同层之间的输入分布变化，以便训练过程更加稳定。

**协变量转移**（Covariate Shift）指的是数据分布在不同阶段发生改变的现象。在神经网络的训练中，协变量转移通常发生在不同层之间，即随着数据从一层流向另一层，其特征分布可能会不断变化。这种变化会导致网络的训练变得不稳定，因为每层都需要去适应前一层特征分布的变化，进而使得梯度更新的效果变差，训练时间也变长。

假设我们正在训练一个神经网络，其中某一层的输出作为下一层的输入。如果前一层的参数在训练过程中不断变化，这一层的输出分布也会变化。下一层接收到的输入特征在每个训练迭代中都可能不同，从而导致不稳定的训练过程。

**批量归一化如何减少协变量转移**

批量归一化通过将每一层的输入特征在每个小批量中进行归一化（标准化到相似的分布），使得特征分布更稳定，避免了因前一层的参数更新而带来的剧烈分布变化。这样，后续层接收到的输入特征不会因为前面层的变化而发生剧烈转移，从而有助于更稳定、高效地训练深度神经网络。

**批量归一化的本质还是在小批量的数据里面加入了随机噪音**

![image-20241108125743698](C:\Users\19409\AppData\Roaming\Typora\typora-user-images\image-20241108125743698.png)

- **公式解释**：这是一个线性变化
  - 公式表示的是批量归一化的计算过程：  
    $
    x_{i+1} = \gamma \frac{x_i - \hat{\mu}_B}{\hat{\sigma}_B} + \beta
    $
    - $ x_i $ 是输入特征；
    - $ \hat{\mu}_B $ 和 $ \hat{\sigma}_B $ 分别是小批量数据的均值和标准差；（这里分别称为“随机缩放”和“随机偏移”）
    - $ \gamma $ 和 $ \beta $ 是可训练的参数，用于控制缩放和偏移；

**无需与丢弃法（Dropout）混合使用**：由于批量归一化引入了一定的正则化效果，因此在某些情况下不必与丢弃法同时使用。

```python
import numpy as np

class BatchNormalization:
    def __init__(self, epsilon=1e-5, momentum=0.9):
        self.epsilon = epsilon  # 防止除以零的小值
        self.momentum = momentum
        self.running_mean = 0
        self.running_var = 0
        self.gamma = None  # 缩放参数
        self.beta = None   # 偏移参数

    def initialize_params(self, input_shape):
        # 初始化缩放参数gamma和偏移参数beta
        self.gamma = np.ones(input_shape)
        self.beta = np.zeros(input_shape)

    def forward(self, x, is_training=True):
        if self.gamma is None or self.beta is None:
            self.initialize_params(x.shape[1])

        if is_training:
            # 计算当前小批量数据的均值和方差
            batch_mean = np.mean(x, axis=0)
            batch_var = np.var(x, axis=0)

            # 归一化
            x_normalized = (x - batch_mean) / np.sqrt(batch_var + self.epsilon)

            # 更新全局均值和方差
            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * batch_mean
            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * batch_var
        else:
            # 使用训练过程中计算的均值和方差
            x_normalized = (x - self.running_mean) / np.sqrt(self.running_var + self.epsilon)

        # 应用缩放和偏移
        out = self.gamma * x_normalized + self.beta
        return out

# 测试批量归一化
x = np.array([[1, 2], [3, 4], [5, 6]])  # 一个小批量数据
bn = BatchNormalization()
output = bn.forward(x)
print("批量归一化后的输出：\n", output)

```

## Dropout

- 一个好的模型需要对输入数据的扰动鲁棒

- 使用有噪音的数据等价于Tikhonov 正则
- 丢弃法：在层之间加入噪音

![image-20241108131548850](C:\Users\19409\AppData\Roaming\Typora\typora-user-images\image-20241108131548850.png)

**Hinton认为：随机采样一些子神经网络，在整体上对网络做了平均化，这样效果会更好**

**丢弃法通常在试验上认为与正则效果一致**

**dropout层**

![image-20241108131648109](C:\Users\19409\AppData\Roaming\Typora\typora-user-images\image-20241108131648109.png)