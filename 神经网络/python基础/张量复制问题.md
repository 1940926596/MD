## x.detach

在 PyTorch 中，`x.detach()` 用于从计算图中分离张量 `x`，从而创建一个新的张量，该张量的梯度计算与原张量断开连接，但它仍然共享相同的数据存储。这意味着 `x.detach()` 生成的张量具有以下属性：

------

### **保留的信息**

1. **数据内容**

   - `x.detach()` 会保留张量的所有数据（即张量中的数值）。
   - 克隆后的数据存储与原张量共享，因此不会引起额外的内存开销。
   - 修改 `detach()` 后张量的值会影响原张量，反之亦然（前提是未使用 `.clone()`）。

   ```python
   import torch
   x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
   y = x.detach()
   print(y)  # tensor([1.0, 2.0, 3.0])
   ```

2. **形状（Shape）**

   - `x.detach()` 返回的张量与原张量具有相同的形状。

   ```python
   print(y.shape)  # torch.Size([3])
   ```

3. **数据类型（Dtype）**

   - 返回张量的数据类型（如 `torch.float32`, `torch.int64`）与原张量一致。

   ```python
   print(y.dtype)  # torch.float32
   ```

4. **设备信息（Device）**

   - 分离后的张量与原张量位于同一个设备（如 CPU 或 GPU）。

   ```python
   print(y.device)  # e.g., cuda:0 or cpu
   ```

5. **梯度跟踪状态（Requires Grad）**

   - `detach()` 返回的张量的 `requires_grad` 属性被设置为 `False`，即分离后的张量不会参与梯度计算。
   - 原张量的 `requires_grad` 属性保持不变。

   ```python
   print(x.requires_grad)  # True
   print(y.requires_grad)  # False
   ```

6. **共享内存**

   - `detach()` 返回的张量与原张量共享内存。这意味着两者的数据内容是**同一块内存**，但它们的梯度信息是分离的。

   - 修改数据会相互影响：

     ```python
     y[0] = 10.0
     print(x)  # tensor([10.0, 2.0, 3.0], requires_grad=True)
     ```

------

### **不保留的信息**

1. **计算图（Computation Graph）**

   - `detach()` 会切断返回张量与原计算图的连接，即分离后的张量不再有与原张量的计算历史。
   - 新张量成为一个**叶子节点**，且与计算图无关。

   ```python
   z = x * 2  # z 与 x 在同一计算图中
   y = x.detach()
   w = y * 2  # w 不在 x 的计算图中
   print(w.requires_grad)  # False
   ```

2. **梯度（Gradient）**

   - 分离后的张量不会保留原张量的 `grad` 属性，`grad` 默认为 `None`。
   - 即便原张量有梯度，分离后的张量也无法访问该梯度。

   ```python
   x.sum().backward()
   print(x.grad)  # tensor([1., 1., 1.])
   print(y.grad)  # None
   ```

------

### **总结**

使用 `x.detach()` 后，返回的张量保留了数据内容、形状、数据类型、设备信息等属性，但分离了计算图，不再参与梯度计算，也无法访问梯度信息。**如果需要完全独立的副本，可以结合 `clone()` 使用**：

```python
z = x.detach().clone()
```

**梯度不保留，梯度属性全部剔除，但是张量的值共享**

## x.clone

在 PyTorch 中，`tensor.clone()` 方法用于创建一个张量的**深拷贝**，它会克隆以下属性：

### 1. **数据内容**

- `tensor.clone()` 会拷贝张量的**数据**，包括其存储的数据和所有元素的值。
- 克隆后的张量与原张量的数据存储位置**完全独立**，修改克隆张量不会影响原张量，反之亦然。

### 2. **形状（Shape）**

- 克隆张量的形状与原张量的形状完全一致。
- 例如：原张量形状为 `(3, 4)`，克隆后张量的形状也会保持为 `(3, 4)`。

### 3. **数据类型（Dtype）**

- 克隆张量的数据类型与原张量相同，例如 `torch.float32` 或 `torch.int64`。

### 4. **设备信息（Device）**

- 克隆张量所在的设备与原张量一致（如 CPU 或 GPU）。

- 如果需要将张量移动到其他设备，可以结合 

  ```
  .to()
  ```

   使用，例如：

  ```python
  new_tensor = tensor.clone().to('cuda')
  ```

### 5. **梯度跟踪设置（Requires Grad）**

- 克隆张量会保留 `requires_grad` 属性。

- 如果原张量设置了 

  ```
  requires_grad=True
  ```

  ，克隆后的张量也会保持这一属性。

  ```python
  import torch
  x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
  y = x.clone()
  print(y.requires_grad)  # True
  ```

### 6. **稀疏格式（若适用）**

- 如果张量是稀疏张量（如 `torch.sparse_coo_tensor`），克隆会保留稀疏格式。

------

### 不会被克隆的属性

1. **梯度（Gradient）**

   - 克隆操作不会复制 `grad`（即梯度）信息，克隆后的张量的 `grad` 默认为 `None`。

   - 示例：

     ```python
     x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
     x.sum().backward()
     print(x.grad)  # tensor([1., 1., 1.])
     
     y = x.clone()
     print(y.grad)  # None
     ```

2. **历史计算图（Computation Graph）**

   - `tensor.clone()` 不会复制原张量的计算历史。
   - 克隆后张量会作为一个**新的叶子节点**存在，且与原张量的计算图无关。

------

### 总结

`tensor.clone()` 会克隆张量的**数据内容、形状、数据类型、设备、requires_grad** 等主要属性，但不会克隆**梯度信息**和**计算历史**。

**梯度保留，但是是不同的值，梯度也是一样的，而且不是叶子节点了**

## AccumulateGrad

在 PyTorch 的计算图中，**`AccumulateGrad`** 是一种与张量相关的钩子，用于处理梯度的累积和计算，通常出现在启用了梯度计算的变量中。

### 原因分析

1. **启用梯度计算时 (`requires_grad=True`)**： 当你创建一个张量并设置 `requires_grad=True` 时，PyTorch 会为该张量关联一个**梯度累积器**，它负责存储和更新这个张量的梯度。这种行为是计算图的一部分，用于支持自动微分。
2. **梯度累积器的功能**：
   - 计算反向传播时，梯度是逐步计算的，并在各操作的计算图中向后传递。
   - 每个张量需要一个地方来存储其反向传播过程中计算得到的梯度，这个存储过程由 `AccumulateGrad` 完成。
   - 如果有多个路径通向同一个张量（如在分支中共享张量的情况），`AccumulateGrad` 会自动将来自这些路径的梯度进行累积。
3. **表现形式**： 当你检查一个启用了梯度计算的张量的 `grad_fn` 属性时，如果这个张量是计算图的**叶子节点**（即由用户创建的，且没有通过计算产生），它的 `grad_fn` 通常会显示为 `AccumulateGrad`。这是因为 PyTorch 需要将该张量的梯度保存在 `grad` 属性中，而这由 `AccumulateGrad` 提供支持。

### 示例

```python
import torch

# 创建一个叶子节点张量
x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)

# 打印它的 grad_fn 属性
print(x.grad_fn)  # 输出: None，因为 x 是叶子节点

# 参与计算，生成新张量
y = x * 2

# 打印 y 的 grad_fn 属性
print(y.grad_fn)  # 输出: <MulBackward0> 表示由乘法操作产生

# 执行反向传播
y.sum().backward()

# 打印 x 的梯度
print(x.grad)  # 输出: tensor([2., 2., 2.])，表示梯度已经累计到 x 的 grad 中
```

在这个例子中，虽然 `x.grad_fn` 是 `None`（叶子节点），但其内部实际上有一个 `AccumulateGrad` 机制来处理 `x.grad` 的存储。

**总结**

- `AccumulateGrad` 是 PyTorch 自动微分系统中不可见的幕后工作者，用于在反向传播中累积和存储梯度。
- 它通常出现在需要保存梯度的叶子节点张量中。
- 如果你不关心梯度计算（例如在推理阶段），可以通过 `with torch.no_grad()` 禁用计算图，从而避免与 `AccumulateGrad` 相关的开销。