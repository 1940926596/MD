## MoE

**混合专家模型** (Mixed Expert Models，简称 MoEs) ，最早是随着 Mixtral 8x7B 的推出而逐渐引起人们的广泛关注。 最近随着DeepSeek的爆火，MoE又进一步引起大家的关注。 本文作者将带你了解 MoEs 的发展史、核心组件、训练方法，推理中各因素考量和DeepSeek MoE详解。

## MoE架构模型

大模型时代，模型规模是提升模型性能的关键因素之一。在有限的计算资源预算下，用更少的训练步数训练一个更大的模型，往往比用更多的步数训练一个较小的模型效果更佳。

**混合专家模型 (MoE) 的一个显著优势是它们能够在远少于稠密模型所需的计算资源下进行有效的预训练**。这意味着在相同的计算预算条件下，您可以显著扩大模型或数据集的规模。特别是在预训练阶段，与稠密模型相比，混合专家模型通常能够更快地达到相同的质量水平。

那么，究竟什么是一个混合专家模型 (MoE) 呢？作为一种基于 Transformer 架构的模型，混合专家模型主要由两个关键部分组成。

- **稀疏 [MoE 层](https://zhida.zhihu.com/search?content_id=255251579&content_type=Article&match_order=1&q=MoE+层&zhida_source=entity)**: 这些层代替了传统 Transformer 模型中的前馈网络 (FFN) 层。MoE 层包含若干“专家”(例如 8 个)，每个专家本身是一个独立的神经网络。在实际应用中，这些专家通常是前馈网络 (FFN)，但它们也可以是更复杂的网络结构，甚至可以是 MoE 层本身，从而形成层级式的 MoE 结构。
- **门控网络或路由**: 这个部分用于决定哪些Token (token) 被发送到哪个专家。例如，在下图中，“More”这个Token可能被发送到第二个专家，而“Parameters”这个Token被发送到第一个专家。有时，一个Token甚至可以被发送到多个专家。Token的路由方式是 MoE 使用中的一个关键点，因为路由器由学习的参数组成，并且与网络的其他部分一同进行预训练。