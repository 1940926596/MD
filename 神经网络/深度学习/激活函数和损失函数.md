## 激活函数

激活函数是神经网络中非常重要的组成部分之一，其作用是将神经元的输入信号转换为输出信号，常见的激活函数包括：

1. Sigmoid函数：Sigmoid函数是一个常用的激活函数，其将输入信号映射到0到1的范围内，常用于二分类问题中。
2. ReLU函数：ReLU函数是一个非常简单且常用的激活函数，其将所有小于0的输入信号设为0，大于等于0的信号不变，可以加速神经网络的训练速度。
3. Tanh函数：Tanh函数与Sigmoid函数类似，其将输入信号映射到-1到1的范围内，常用于分类和回归问题中。
4. Softmax函数：Softmax函数是一种特殊的激活函数，常用于多分类问题中，将多个输出信号映射到0到1的范围内，并满足输出信号之和为1。

选择激活函数的主要目的是引入非线性特性，使得神经网络可以学习更加复杂的模式和关系。不同的激活函数在不同的场景中表现不同，需要根据具体的任务和数据集来选择合适的激活函数。

## 损失函数

损失函数是神经网络中非常重要的组成部分之一，其作用是衡量模型预测结果与真实结果之间的差异，常见的损失函数包括：

1. 均方误差（Mean Squared Error，MSE）：均方误差是回归任务中最常用的损失函数，其计算方法是预测结果与真实结果之间差值的平方和的平均值。
2. 交叉熵（Cross Entropy）：交叉熵是分类任务中最常用的损失函数，其计算方法是真实标签和预测标签之间的交叉熵。
3. 对数损失（Log Loss）：对数损失是二分类任务中常用的损失函数，其计算方法是真实标签和预测标签之间的对数差。
4. KL散度（Kullback-Leibler Divergence）：KL散度是衡量两个概率分布之间的差异的一种指标，常用于对比两个分布之间的相似度。
5. Huber Loss：Huber Loss是一种平滑的损失函数，可以在回归任务中比均方误差更加鲁棒。

选择损失函数的主要目的是最小化预测结果与真实结果之间的差异，使得模型能够更加准确地预测目标值。不同的损失函数在不同的场景中表现不同，需要根据具体的任务和数据集来选择合适的损失函数。