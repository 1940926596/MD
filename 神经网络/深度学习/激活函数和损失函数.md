# 激活函数和损失函数
## 激活函数

激活函数是神经网络中非常重要的组成部分之一，其作用是将神经元的输入信号转换为输出信号，常见的激活函数包括：

1. Sigmoid函数：Sigmoid函数是一个常用的激活函数，其将输入信号映射到0到1的范围内，常用于二分类问题中。
2. ReLU函数：ReLU函数是一个非常简单且常用的激活函数，其将所有小于0的输入信号设为0，大于等于0的信号不变，可以加速神经网络的训练速度。
3. Tanh函数：Tanh函数与Sigmoid函数类似，其将输入信号映射到-1到1的范围内，常用于分类和回归问题中。
4. Softmax函数：Softmax函数是一种特殊的激活函数，常用于多分类问题中，将多个输出信号映射到0到1的范围内，并满足输出信号之和为1。

选择激活函数的主要目的是引入非线性特性，使得神经网络可以学习更加复杂的模式和关系。不同的激活函数在不同的场景中表现不同，需要根据具体的任务和数据集来选择合适的激活函数。

**如何选择激活函数？**

**隐藏层**
- 优先选择RELU激活函数
- 如果ReLu效果不好，那么尝试其他激活，如Leaky ReLu等。
- 如果你使用了Relu， 需要注意一下Dead Relu问题， 避免出现大的梯度从而导致过多的神经元死亡。
- 不要使用sigmoid激活函数，可以尝试使用tanh激活函数

**输出层**

- 二分类问题选择sigmoid激活函数
- 多分类问题选择softmax激活函数
- 回归问题选择identity激活函数

---

==选择激活函数时，依据问题类型和模型特性来进行选择。以下是对上述选择激活函数的建议的依据与解释：==

### 隐藏层
1. **ReLU (Rectified Linear Unit)**:
   - **优先选择ReLU**：ReLU 是目前最常用的激活函数，因为它能够有效地解决梯度消失问题，并且计算简单。ReLU 在大多数情况下表现出色，因此是默认选择。
   - **避免Dead ReLU问题**：ReLU 的缺点是可能导致“Dead ReLU”问题，即在训练过程中某些神经元可能永远不会被激活（输出恒为零），特别是在权重初始化不当或学习率较大时。为了避免这种情况，可以尝试使用改进版的ReLU，比如 Leaky ReLU 或 Parametric ReLU。

2. **Leaky ReLU**:
   - 如果 ReLU 导致了过多的“Dead ReLU”，可以尝试 Leaky ReLU 或 Parametric ReLU，这些变体允许负输入有一个小的负斜率，从而减少“神经元死亡”的可能性。

3. **Sigmoid**:
   - **不推荐在隐藏层使用**：Sigmoid 激活函数在处理深度网络时容易导致梯度消失问题，因为它的导数在输入值非常大或非常小时趋近于零，导致梯度更新非常慢。因此，通常不建议在隐藏层使用。

4. **Tanh**:
   - **可尝试使用 Tanh**：Tanh 激活函数在 Sigmoid 的基础上进行了改进，其输出范围在 [-1, 1] 之间，这种特性使得 Tanh 在有些情况下优于 Sigmoid，特别是在深度网络中，它能够缓解一定程度的梯度消失问题。

### 输出层
1. **Sigmoid (二分类问题)**:
   - **适用于二分类问题**：Sigmoid 激活函数将输出映射到 [0, 1] 范围内，可以解释为概率，因此在二分类问题中是常用的输出激活函数。

2. **Softmax (多分类问题)**:
   - **适用于多分类问题**：Softmax 激活函数将多个输出节点的值转换为概率分布（所有输出的概率之和为 1）。因此，在多分类问题中，Softmax 是最常用的输出激活函数。

3. **Identity (回归问题)**:
   - **适用于回归问题**：在回归问题中，输出层的任务是预测一个连续的数值。因此，不需要将输出值映射到某个范围内，通常使用线性激活（即 identity 函数），这意味着输出值直接等于输入值。

### 总结：
- **隐藏层**：ReLU 是默认的激活函数，如果效果不佳或遇到“Dead ReLU”问题，可以尝试 Leaky ReLU 或其他变体。避免使用 Sigmoid，Tanh 作为备选方案。
- **输出层**：根据问题类型选择激活函数，二分类问题使用 Sigmoid，多分类问题使用 Softmax，回归问题使用线性激活。

## 损失函数

损失函数是神经网络中非常重要的组成部分之一，其作用是衡量模型预测结果与真实结果之间的差异，常见的损失函数包括：

1. 均方误差（Mean Squared Error，MSE）：均方误差是回归任务中最常用的损失函数，其计算方法是预测结果与真实结果之间差值的平方和的平均值。
2. 交叉熵（Cross Entropy）：交叉熵是分类任务中最常用的损失函数，其计算方法是真实标签和预测标签之间的交叉熵。
3. 对数损失（Log Loss）：对数损失是二分类任务中常用的损失函数，其计算方法是真实标签和预测标签之间的对数差。
4. KL散度（Kullback-Leibler Divergence）：KL散度是衡量两个概率分布之间的差异的一种指标，常用于对比两个分布之间的相似度。
5. Huber Loss：Huber Loss是一种平滑的损失函数，可以在回归任务中比均方误差更加鲁棒。

选择损失函数的主要目的是最小化预测结果与真实结果之间的差异，使得模型能够更加准确地预测目标值。不同的损失函数在不同的场景中表现不同，需要根据具体的任务和数据集来选择合适的损失函数。